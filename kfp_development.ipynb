{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498d2e25-fc22-4266-8113-d29699be1da8",
   "metadata": {},
   "source": [
    "## In this Notebook we are developing all the components that are required to run the anomaly detection job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c978e63a-c3f0-40ef-b867-3dada208ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f8624b5e-1365-4725-9d2d-b4c5a335cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This component detects anomalies in netscout data. \n",
    "\"\"\"\n",
    "But eventually I think we need to have our own table and just query the most recent one from the \n",
    "`prj-netw-prod-npda-dataeng-03.wlssrtsv_netscout_raw.wireless-netscout-control-plane`\n",
    "and save it in our own table. Additionally we also can have a data retention policy (10 days or even less) to avoid costs.\n",
    "\"\"\"\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"fsspec\", \"gcsfs\", \"prophet\", \"google-cloud-bigquery\", \"rrcf\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"netscout-ml.yaml\"\n",
    ")\n",
    "def netscout_ml_w6017_amf_server(project_id: str, dataset: str, table: str, storage_gcs: str ):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    from prophet import Prophet\n",
    "    import rrcf\n",
    "    import json\n",
    "    from typing import Tuple\n",
    "    import io\n",
    "    \n",
    "\n",
    "    def get_recent_transaction(project_id: str, dataset: str, table: str):\n",
    "        \"\"\"\"\n",
    "        Getting the most recent (in the last 10 min) netscout Data\n",
    "        \n",
    "        Inputs: The GCP Project ID, dataset name, table name\n",
    "        \n",
    "        Outputs: A dataframe with the timestamp and the value at that timestamp\n",
    "        \"\"\"\n",
    "        \n",
    "        client = bigquery.Client(project=project_id)\n",
    "\n",
    "        query = \"select destination_host_ip_address from `table_xxxx` limit 10\"\n",
    "        # Execute a query.\n",
    "        QUERY = (query)\n",
    "    \n",
    "        query_job = client.query(QUERY)  # API request\n",
    "        rows = query_job.result()  # Waits for query to finish\n",
    "    \n",
    "        df_recent_trans = pd.DataFrame([dict(row) for row in rows])\n",
    "        print(df_recent_trans.head())\n",
    "        if not df_recent_trans.empty:\n",
    "            df_recent_trans.columns = ['ds', 'y']\n",
    "            df_recent_trans['ds'] = df_recent_trans['ds'].dt.tz_localize(None)\n",
    "            df_recent_trans['y'] = df_recent_trans['y'].astype(float)\n",
    "            print(df_recent_trans.head())\n",
    "        \n",
    "        return df_recent_trans\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_prior_30_days(storage_gcs: str):\n",
    "        \"\"\"\"\n",
    "        Getting the historical data (the last 30 days) from the cloud storage, changing the column names and then removing the UTC from 'ds'\n",
    "        \n",
    "        Inputs: Storage Bucket \n",
    "        \n",
    "        Outputs: A dataframe with the historical data with the timestamp and its value.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path_last_30days = 'preprocessed_success_trans0725_0824.csv'        \n",
    "        df_past_30_days = pd.read_csv(storage_gcs+file_path_last_30days, parse_dates=['Datetime'])\n",
    "        df_past_30_days.columns = ['ds', 'y']\n",
    "        df_past_30_days['ds'] = df_past_30_days['ds'].dt.tz_localize(None)\n",
    "        \n",
    "        return df_past_30_days\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_prophet(historical_df: pd.DataFrame, recent_df: pd.DataFrame):\n",
    "        \"\"\"\"\n",
    "        Train the prophet model on the last 30 days (historical_df) and forecast the next time stamp and compare it with the \n",
    "        recent_df timestamp value, if there is significant difference (i.e. falls outside the uncertainity level of the forecasted\n",
    "        value), then declare it as an anomaly.\n",
    "        \n",
    "        Inputs: Historical Dataframe (the last 30 days) and Most Recent Dataframe\n",
    "        \n",
    "        Outputs: Alarm Notification (True/False) and the updated dataframe (i.e. the oldest point deleted and the newest point added)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prophet model\n",
    "        model = Prophet(growth='flat', seasonality_mode='additive', interval_width=0.7)\n",
    "        model.fit(historical_df)\n",
    "        df_trans_future = model.make_future_dataframe(periods=12, freq='5T', include_history=True)\n",
    "        forecast = model.predict(df_trans_future)\n",
    "        print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n",
    "        print(historical_df.tail())\n",
    "        historical_df_1 = pd.concat([historical_df, recent_df], ignore_index=True)\n",
    "        print(\"before \", historical_df.shape, historical_df_1.shape)\n",
    "        historical_df_1 = historical_df_1.iloc[1:]\n",
    "        print(historical_df_1.shape, \" after\")\n",
    "        print(historical_df_1.tail())\n",
    "        print(historical_df_1.head())\n",
    "        \n",
    "        \n",
    "        # Now let us look into if the observed value is outside the box of the uncertainity level\n",
    "        neg_yhat_lower = forecast['yhat_lower'] < 0\n",
    "        print(len(neg_yhat_lower))\n",
    "        forecast.loc[neg_yhat_lower, 'yhat_lower'] = 0\n",
    "        lower_level = forecast.iloc[-12]['yhat_lower']\n",
    "        upper_level = forecast.iloc[-12]['yhat_upper']\n",
    "        alarm_notification = False\n",
    "        if (recent_df.iloc[0]['y'] <= lower_level) or (upper_level < recent_df.iloc[0]['y']):\n",
    "            alarm_notification = True\n",
    "        \n",
    "        print('alarm notification')\n",
    "        print(lower_level, recent_df.iloc[0]['y'], upper_level)\n",
    "        return (alarm_notification, historical_df_1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def incremental_train_rrcf(recent_df: pd.DataFrame, storage_gcs: str, rrcf_threshold: float):\n",
    "        \"\"\"\"\n",
    "        Update the Robust Random Cut Forest (rrcf) model by adding the most recent point (recent_df). Check if the most recent data points \n",
    "        anomaly score is greater than the given threshold (rrcf_threshold). If so it is an anomaly. Serialize the updated trees and save it\n",
    "        to the storage. \n",
    "        \n",
    "        Inputs: Most Recent Dataframe, Storage (serialized json files), Threshold (RRCF theshold)\n",
    "        \n",
    "        Outputs: Alarm Notification (True/False).\n",
    "        \"\"\"\n",
    "        \n",
    "        forest_retrieved = []\n",
    "        pickle_files_path = 'serialized_files/'\n",
    "        \n",
    "        \n",
    "        client = storage.Client()\n",
    "        bucket_name = 'xxx_bucket'\n",
    "        blob_name = 'serialized_files/'\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        \n",
    "        for i in range(80):\n",
    "            print('Beginning to Retrieve Pickled Files')\n",
    "            blob = bucket.blob(blob_name+'tree_'+str(i)+'.json')\n",
    "            file_like_object = io.BytesIO(blob.download_as_bytes())\n",
    "            #json_data = blob.download_as_file()\n",
    "            obj = json.load(file_like_object)\n",
    "            tree = rrcf.RCTree()\n",
    "            tree.load_dict(obj)\n",
    "            print(tree)\n",
    "            forest_retrieved.append(tree)\n",
    "            \n",
    "            # with open(storage+pickle_files_path+'tree_'+str(i)+'.json', 'r') as infile:\n",
    "            #     obj = json.load(infile)\n",
    "            #     tree = rrcf.RCTree()\n",
    "            #     tree.load_dict(obj)\n",
    "            #     forest_retrieved.append(tree)\n",
    "        \n",
    "        print(len(forest_retrieved))\n",
    "        # Initialize a client\n",
    "        client = storage.Client()\n",
    "\n",
    "        # Define the bucket and file you want to write to\n",
    "        bucket_name = 'xxx_bucket'\n",
    "        base_blob_name = 'updated_xxx/'\n",
    "        count = 0\n",
    "        \n",
    "        for i, tree in enumerate(forest_retrieved):\n",
    "            \n",
    "\n",
    "            # # Get the bucket and blob\n",
    "            # bucket = client.get_bucket(bucket_name)\n",
    "            # blob = bucket.blob(blob_name)\n",
    "\n",
    "            \n",
    "            obj = tree.to_dict()\n",
    "            json_data = json.dumps(obj)\n",
    "\n",
    "            # Create a file-like object\n",
    "            file_like_object = io.BytesIO(json_data.encode())\n",
    "\n",
    "            # Define the blob name (including the 'tree_' prefix)\n",
    "            blob_name = base_blob_name + 'tree_' + str(i) + '.json'\n",
    "\n",
    "            # Upload the file-like object to Google Cloud Storage\n",
    "            bucket = client.get_bucket(bucket_name)\n",
    "            blob = bucket.blob(blob_name)\n",
    "            blob.upload_from_file(file_like_object, content_type='application/json')\n",
    "\n",
    "            print(f'File uploaded to {bucket_name}/{blob_name}')\n",
    "            count += 1\n",
    "            if count == 1:\n",
    "                  break\n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def eval_n_alarm_notification(prophet_result: Tuple[(str, str)], rrcf_result: Tuple[(str, str)]):\n",
    "        \"\"\"\"\n",
    "        Send the alarm notification if both the prophet_result and rrcf_result is True.\n",
    "        \n",
    "        \n",
    "        Inputs: Historical Dataframe (the last 30 days) and Most Recent Dataframe\n",
    "        \n",
    "        Outputs: Alarm Notification (True/False) and the updated dataframe (i.e. the oldest point deleted and the newest point added)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        return\n",
    "    \n",
    "    df_recent_transactions = get_recent_transaction(project_id=project_id, dataset=dataset, table=table)\n",
    "    #print(df_recent_transactions)\n",
    "    if not df_recent_transactions.empty:\n",
    "        df_past_30_days = get_prior_30_days(storage_gcs)\n",
    "        print(df_past_30_days.head())\n",
    "        prophet_alarm_notification, df_30_days_recent = train_prophet(df_past_30_days, df_recent_transactions)\n",
    "        print(prophet_alarm_notification)\n",
    "        incremental_train_rrcf(df_recent_transactions, storage_gcs, 10.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8faa4f82-87cc-4af9-aace-6054f8bdc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline.\n",
    "\n",
    "from datetime import datetime\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "# print(TIMESTAMP) 20230913141236\n",
    "DISPLAY_NAME = 'pipeline-anomaly-detection-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "bc630196-9bb4-4b78-9857-780a88133be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://xxx_bucket\"\n",
    "PIPELINE_ROOT = \"{}/alarm-detection\".format(BUCKET_URI)\n",
    "SERVICE_ACCOUNT = \"name@xxx.iam.gserviceaccount.com\"\n",
    "PROJECT_ID = \"xxx_project_id\"\n",
    "REGION = \"central-us\"\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7e08d948-e1e5-495a-8da8-47a95730a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=DISPLAY_NAME,\n",
    "    description=\"Netscout ML Pipeline Test\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(project_id: str = PROJECT_ID, dataset: str = 'xxx_dataset', table: str = 'xxx_table', storage_gcs: str = 'gs://xxx_bucket'):\n",
    "    netscout_preprocess_task = netscout_ml_w6017_amf_server(project_id, dataset, table, storage_gcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1a1d903b-d837-42cc-84ed-446f66453c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline-anomaly-detection-job.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798cfe56-f6dc-4e1f-9527-f734fe5e720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"pipeline-anomaly-detection-job.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "        service_account=SERVICE_ACCOUNT\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
